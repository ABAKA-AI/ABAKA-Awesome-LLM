# ABAKA AI Awesome LLM

Large Language Mode(LLM) has made striking achievements, and the world of AI assistantsü§ñ for everyone seems to be getting closer and closer to the human race. This is a repository of all kinds of LLMs, and we've divided it into tables with different ways of training the models.

English | [ÁÆÄ‰Ωì‰∏≠Êñá](README_zh.md)



# Overview items

* [ABAKA AI Awesome LLM](#abaka-ai-awesome-llm)
  * [Overview items](#overview-items)
  * [Updates](#updates)
  * [Base Model](#base-model)
  * [SFT Model](#sft-model)
  * [RLHF](#rlhf)
  * [Insight](#insight)
  * [LLM Extensions](#llm-extensions)



## Updates

* [2023-08-07] First created day!!! üéâüéâüéâ



## Base Model

We started by summarizing the Base LLM, which has the benchmark from the [open-llm-leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) link data. AVG stands for average value from benchmark.

|       Model        | parameter size |  Date   |                       Origin & Source                        |                 AVG                  |   Tokens size   | Language |                           License                            |
| :----------------: | :------------: | :-----: | :----------------------------------------------------------: | :----------------------------------: | :-------------: | :------: | :----------------------------------------------------------: |
| Switch Transformer |     1.6 T      | 2021-01 |    [paper](https://arxiv.org/pdf/2101.03961.pdf) \| none     |                                      |      5.3 B      | En most  |                                                              |
|        GLaM        |     1.2 T      | 2021-12 |    [paper](https://arxiv.org/pdf/2112.06905.pdf) \| none     |                                      |      1.6 T      | En most  |                                                              |
|        PaLM        |     540 B      | 2022-04 |    [paper](https://arxiv.org/pdf/2204.02311.pdf) \| none     |                                      |      780 B      | En most  |                                                              |
|       MT-NLG       |     530 B      | 2022-01 |    [paper](https://arxiv.org/pdf/2201.11990.pdf) \| none     |                                      |      338 B      | En most  |                                                              |
|      J1-Jumbo      |     178 B      | 2021-08 | [paper](https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf) \| none |                                      |      300 B      | En most  |                                                              |
|        OPT         |     175 B      | 2022-05 | [paper](https://arxiv.org/pdf/2205.01068.pdf) \| [ckpt](https://github.com/facebookresearch/metaseq/tree/main/projects/OPT) |                                      |      180 B      | En most  | [OPT-175B](https://github.com/facebookresearch/metaseq/blob/edefd4a00c24197486a3989abe28ca4eb3881e59/projects/OPT/MODEL_LICENSE.md) |
|       BLOOM        |     176 B      | 2022-11 | [paper](https://arxiv.org/pdf/2211.05100.pdf) \| [ckpt](https://huggingface.co/bigscience/bloom) |                                      |      350 B      | En most  | [ License](https://huggingface.co/spaces/bigscience/license) |
|      GPT 3.0       |     175 B      | 2020-05 |    [paper](https://arxiv.org/pdf/2005.14165.pdf) \| none     |                                      |      499 B      | En most  |                                                              |
|       LaMDA        |     137 B      | 2022-01 |    [paper](https://arxiv.org/pdf/2201.08239.pdf) \| none     |                                      |     2.81 T      | En most  |                                                              |
|        GLM         |     130 B      | 2022-10 | [paper](https://arxiv.org/pdf/2210.02414.pdf) \| [ckpt](https://github.com/THUDM/GLM-130B) |                                      |      400 B      | En most  | [GLM-130B](https://github.com/THUDM/GLM-130B/blob/799837802264eb9577eb9ae12cd4bad0f355d7d6/MODEL_LICENSE) |
|        YaLM        |     100 B      | 2022-06 | [blog](https://medium.com/yandex/yandex-publishes-yalm-100b-its-the-largest-gpt-like-neural-network-in-open-source-d1df53d0e9a6) \| [ckpt](https://github.com/yandex/YaLM-100B#downloading-checkpoint) |                                      | 1.7 TB(storage) | En & Ru  | [Apache 2.0](https://github.com/yandex/YaLM-100B/blob/14fa94df2ebbbd1864b81f13978f2bf4af270fcb/LICENSE) |
|       LLaMA        |  7/13/33/65 B  | 2022-09 | [paper](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/) \| [ckpt](https://github.com/facebookresearch/llama#download) | 7B: 49.7<br/>33B: 61.7<br/>65B: 62.1 |      1.4 T      | En most  | [Apacch 2.0](https://github.com/yandex/YaLM-100B/blob/main/LICENSE) |
|      GPT-NeoX      |      20 B      | 2022-04 | [paper](https://arxiv.org/pdf/2204.06745.pdf) \| [ckpt](https://github.com/EleutherAI/gpt-neox#pretrained-models) |                 46.4                 |     825 GiB     | En most  | [Apache 2.0](https://github.com/EleutherAI/gpt-neox/blob/main/LICENSE) |
|       Falcon       |      40 B      | 2023-05 | [homepage](https://falconllm.tii.ae/) \| [ckpt](https://huggingface.co/tiiuae/falcon-40b) |                 61.5                 |       1 T       |          | [Apache 2.0](https://github.com/EleutherAI/gpt-neox/blob/main/LICENSE) |
|        UL2         |      20 B      | 2022-05 | [paper](https://arxiv.org/pdf/2205.05131v1.pdf) \| [ckpt](https://huggingface.co/google/ul2) |                                      |      32 B       | En most  | [Apache 2.0](https://github.com/EleutherAI/gpt-neox/blob/main/LICENSE) |
|     ÈπèÁ®ã.ÁõòÂè§Œ±     |      13 B      | 2021-04 | [paper](https://arxiv.org/pdf/2104.12369.pdf) \| [ckpt](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/PanGu-Œ±#Ê®°Âûã‰∏ãËΩΩ) |                                      |     26.5 B      | En most  | [Apache 2.0](https://github.com/EleutherAI/gpt-neox/blob/main/LICENSE) |
|         T5         |      11 B      | 2019-10 | [paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf) \| [ckpt](https://huggingface.co/t5-11b) |                                      |      34 B       | En most  | [Apache 2.0](https://github.com/EleutherAI/gpt-neox/blob/main/LICENSE) |
|      CPM-Bee       |      10 B      | 2022-10 |    [paper](https://arxiv.org/pdf/2012.00413.pdf) \| none     |                                      |                 | Ch & En  | [CPM-Bee](https://github.com/OpenBMB/General-Model-License/blob/main/ÈÄöÁî®Ê®°ÂûãËÆ∏ÂèØÂçèËÆÆ-Êù•Ê∫êËØ¥Êòé-ÂÆ£‰º†ÈôêÂà∂-ÂïÜ‰∏öÊéàÊùÉ.md) |
|       rwkv-4       |      7 B       | 2022-09 | [paper](https://arxiv.org/pdf/2305.13048.pdf) \| [ckpt](https://huggingface.co/BlinkDL/rwkv-4-pile-7b) |                                      |                 |          |                                                              |
|       GPT-J        |      6 B       | 2022-09 | none \| [ckpt](https://github.com/kingoflolz/mesh-transformer-jax#pretrained-models) |                 42.8                 |      400 B      | En most  | [Apache 2.0](https://github.com/EleutherAI/gpt-neox/blob/main/LICENSE) |
|      GPT-Neo       |     2.7 B      | 2021-03 | none \| [ckpt](https://github.com/EleutherAI/gpt-neo#pretrained-models) |                 38.9                 |                 |          | [MIT](https://github.com/EleutherAI/gpt-neo/blob/23485e3c7940560b3b4cb12e0016012f14d03fc7/LICENSE) |
|    baichuan-7B     |      7 B       | 2023-06 | [github](https://github.com/baichuan-inc/baichuan-7B) \| [ckpt](https://huggingface.co/baichuan-inc/Baichuan-7B) |                                      |      1.2 T      |          |                                                              |



## SFT Model

|     Model      | parameter size |  Date   |                       Origin & Source                        |                AVG                 | Tokens size | Language |                           License                            |
| :------------: | :------------: | :-----: | :----------------------------------------------------------: | :--------------------------------: | :---------: | :------: | :----------------------------------------------------------: |
|    StableLM    |  3/7/15/65 B   | 2023-04 | [homepage](https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models) \| none |       3B: 34.3<br/>7B: 38.7        |    1.5 T    | En most  | [Apache 2.0](https://github.com/EleutherAI/gpt-neox/blob/main/LICENSE) |
|   Flan-PaLM    |     540 B      | 2022-10 |    [paper](https://arxiv.org/pdf/2210.11416.pdf) \| none     |                                    |    1.4 B    |          |                                                              |
|     BLOOMZ     |     176 B      | 2022-11 | [paper](https://arxiv.org/pdf/2211.01786.pdf) \| [ckpt](https://huggingface.co/bigscience/bloomz) |                                    |   2.09 B    | En most  | [License](https://huggingface.co/spaces/bigscience/license)  |
|  InstructGPT   |     175 B      | 2022-03 |    [paper](https://arxiv.org/pdf/2203.02155.pdf) \| none     |                                    |  13 K item  | En most  |                                                              |
|   Galactica    |     120 B      | 2022-11 | [paper](https://arxiv.org/pdf/2211.09085.pdf) \| [ckpt](https://huggingface.co/facebook/galactica-120b) |                                    |    106 B    | En most  |                                                              |
|  OpenChatKit   |      20 B      | 2023-3  | [github](https://github.com/togethercomputer/OpenChatKit) \| [ckpt](https://huggingface.co/togethercomputer/Pythia-Chat-Base-7B) |                                    |     1 T     | En most  | [Apache 2.0](https://github.com/EleutherAI/gpt-neox/blob/main/LICENSE) |
|    Flan-UL2    |      20 B      | 2023-03 | [homepage](https://www.yitay.net/blog/flan-ul2-20b) \| [ckpt](https://huggingface.co/google/flan-ul2) |                49.1                |             |          |    [ Apache 2.0](https://huggingface.co/google/flan-ul2)     |
|     Gopher     |     280 B      |         | [paper](https://storage.googleapis.com/deepmind-media/research/language-research/Training%20Gopher.pdf) \| none |                                    |    300 B    |          |                                                              |
|   Chinchilla   |      70 B      |         |    [paper](https://arxiv.org/pdf/2203.15556.pdf) \| none     |                                    |    1.4 T    |          |                                                              |
|    Flan-T5     |      11 B      | 2022-10 | [paper](https://arxiv.org/pdf/2210.11416.pdf) \| [ckpt](https://huggingface.co/google/flan-t5-base) |                                    |    100 B    |          | [Apache 2.0](https://github.com/google-research/t5x/blob/776279bdacd8c5a2d3e8ce0f2e7064bd98e98b47/LICENSE) |
|       T0       |      11 B      | 2021-10 | [paper](https://arxiv.org/pdf/2110.08207.pdf) \| [ckpt](https://huggingface.co/bigscience/T0) |                                    |     1 T     |          |      [Apache 2.0](https://huggingface.co/bigscience/T0)      |
|     Alpaca     |      7 B       | 2023-03 | [github](https://github.com/tatsu-lab/stanford_alpaca) \| none |                31.9                |  52 K item  |          | [Apache 2.0](https://github.com/tatsu-lab/stanford_alpaca/blob/main/LICENSE) |
|      Orca      |      13 B      | 2023-06 |    [paper](https://arxiv.org/pdf/2306.02707.pdf) \| none     |                57.7                |  5 M item   |          | [License](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md) |
|   ChatGLM-6B   |      6 B       | 2023-03 | [github](https://github.com/THUDM/ChatGLM-6B) \| [ckpt](https://huggingface.co/THUDM/chatglm-6b) |                                    |     1 T     | Ch & En  | [Apache 2.0](https://github.com/THUDM/ChatGLM-6B/blob/main/LICENSE) |
|  ChatGLM2-6B   |      6 B       | 2023-06 | [github](https://github.com/THUDM/ChatGLM2-6B) \| [ckpt](https://huggingface.co/THUDM/chatglm2-6b) |                48.2                |    1.4 T    | Ch & En  |  [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0)   |
|      Ziya      |      13 B      | 2023-05 | [github](https://github.com/IDEA-CCNL/Fengshenbang-LM) \| [ckpt](https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-v1.1) |                32.3                |    125 B    | Ch & En  | [Apache 2.0](https://github.com/IDEA-CCNL/Fengshenbang-LM/blob/main/LICENSE) |
|    Phonenix    |      7 B       | 2023-04 | [github](https://github.com/FreedomIntelligence/LLMZoo) \| [ckpt](https://github.com/FreedomIntelligence/LLMZoo#phoenix-llm-across-languages) |                                    | 922 M item  | Ch & En  | [Apache 2.0](https://github.com/FreedomIntelligence/LLMZoo/blob/main/LICENSE) |
|    Dolly2.0    |      12 B      | 2023-04 | [blog](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm) \| [ckpt](https://huggingface.co/databricks/dolly-v2-12b) |                43.7                |  15 K item  |    En    | [Apache 2.0](https://github.com/databrickslabs/dolly/blob/master/LICENSE) |
|     Dolly      |      6 B       | 2023-03 | [github](https://github.com/databrickslabs/dolly) \| [ckpt](https://huggingface.co/databricks/dolly-v1-6b) |                                    |  52 K item  |    En    | [Apache 2.0](https://github.com/databrickslabs/dolly/blob/master/LICENSE) |
|    GALPACA     |      30 B      | 2022-11 | [paper](https://galactica.org/static/paper.pdf) \| [ckpt](https://huggingface.co/GeorgiaTechResearchInstitute/galpaca-30b) |                48.2                |  52 K item  |    En    | [CC By NC 4.0](https://github.com/tatsu-lab/stanford_alpaca/blob/main/DATA_LICENSE) |
|    UltraLM     |      13 B      | 2023-03 | [github](https://github.com/thunlp/UltraChat) \| [ckpt](https://huggingface.co/openbmb/UltraLM-13b) |                60.3                | 774 K item  |    En    | [CC By NC 4.0](https://github.com/thunlp/UltraChat/blob/main/LICENSE) |
|    Guanaco     |  7/13/33/65 B  | 2023-03 | [github](https://github.com/artidoro/qlora) \| [ckpt](https://huggingface.co/timdettmersc) | 7B: 56.2<br/>13B: 59.1<br/>33B: 63 | 9.85 K item |    En    | [MIT License](https://github.com/artidoro/qlora/blob/main/LICENSE) |
|    BayLing     |      13 B      | 2023-06 | [github](https://github.com/ictnlp/BayLing) \| [ckpt](https://huggingface.co/ICTNLP/bayling-13b-v1.1) |                                    | 160 K item  | Ch & En  | [GPL:v3](https://github.com/ictnlp/BayLing/blob/main/LICENSE) |
|     KnowLM     |      13 B      | 2023-06 | [github](https://github.com/zjunlp/KnowLM/) \| [ckpt](https://github.com/zjunlp/KnowLM/#2-2) |                                    | 1400 K item | Ch & En  | [Apache 2.0](https://github.com/zjunlp/KnowLM/blob/main/LICENSE) |
|    WizardLM    |    13/30 B     | 2023-06 | [github](https://github.com/nlpxucan/WizardLM) \| [ckpt](https://huggingface.co/WizardLM) |      13B: 60.4<br/>30B: 62.9       | 143 K item  |    En    | [Apache 2.0](https://github.com/tatsu-lab/stanford_alpaca/blob/main/LICENSE) |
|     BELLE      |      7 B       | 2023-04 | [github](https://github.com/LianjiaTech/BELLE) \| [ckpt](https://huggingface.co/BelleGroup/BELLE-7B-2M) |                                    |  10 M item  |          | [Apache 2.0](https://github.com/LianjiaTech/BELLE/blob/main/LICENSE) |
|     Koala      |      13 B      | 2023-04 | [homepage](https://bair.berkeley.edu/blog/2023/04/03/koala/) \| [ckpt](https://huggingface.co/TheBloke/koala-13B-HF) |                56.5                | 420 K item  |    En    |                                                              |
| Chinese-Vicuna |     7/13 B     | 2023-03 | [github](https://github.com/Facico/Chinese-Vicuna) \| [ckpt](https://huggingface.co/Facico) |                                    | 0.5 M item  | Ch & En  | [Apache 2.0](https://github.com/Facico/Chinese-Vicuna/blob/master/LICENSE) |
|     Baize      |   7/13/30 B    | 2023-03 | [github](https://github.com/project-baize/baize-chatbot) \| [ckpt](https://huggingface.co/project-baize) |       7B: 51.3<br/>13B: 58.4       | 100 K item  | Ch & En  | [License](https://github.com/project-baize/baize-chatbot/blob/main/LICENSE) |
|      MOSS      |      7 B       | 2023-04 | [github](https://github.com/OpenLMLab/MOSS) \| [ckpt](https://huggingface.co/fnlp/moss-moon-003-sft) |                                    |             |          | [LICENSE](https://github.com/OpenLMLab/MOSS/blob/main/MODEL_LICENSE) |



## RLHF

|   Model    | parameter size |  Date   |                          Origin                           | tokens size  |
| :--------: | :------------: | :-----: | :-------------------------------------------------------: | :----------: |
|   GPT 4    |     1.8 T      | 2023-03 |         [blog](https://openai.com/research/gpt-4)         |     13 T     |
|  ChatGPT   |     175 B      |         |                                                           |              |
|  Sparrow   |      70 B      |         |                                                           |              |
|   Claude   |                |         |                                                           |              |
| StackLLaMA |      7 B       |         | [hugging face](https://huggingface.co/blog/zh/stackllama) | 10.8 M items |



## Insight

We have organized the outstanding papers in the previous model and summarized their insight (innovations).

| Model              | Link                                                         | Insight                                                      |
| ------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Switch Transformer | [paper](https://arxiv.org/pdf/2101.03961.pdf)                | MoE hybrid expert models with ridiculously high parameter counts are also hampered by complexity, communication costs, and training instability; this thesis introduces a Switch transformer to address these issues, simplifies the MoE routing algorithm, designs intuitively improved models, reduces communication and computational costs, and demonstrates for the first time that a large sparse model can be trained using a lower precision (bfloat16) format. |
| GLaM               | [paper](https://arxiv.org/pdf/2112.06905.pdf)                | Training large, dense models requires significant computational resources, and the article presents a family of language models for GLaM, which uses an expert mixture architecture with sparse activation to scale model capacity. The largest GLaM has 1.2 trillion parameters, which is about 7 times the size of GPT-3. It consumes only 1/3 the energy of training GPT-3 and requires half the computation for inference, but still achieves better zero, one, and few overall performance on 29 NLP tasks. |
| PaLM               | [paper](https://arxiv.org/pdf/2204.02311.pdf)                | The article provides further insight into the impact of scale on few-shot learning, demonstrating the continued benefits of scaling by achieving state-of-the-art lesser learning results on hundreds of language comprehension and generation benchmarks. |
| MT-NLG             | [paper](https://arxiv.org/pdf/2201.11990.pdf)                | The article details the training details of the converter-based maximal monolithic language model Megatron-Turing NLG 530B (MT-NLG) with 530 billion parameters. |
| J1-Jumbo           | [paper](https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf) | The article describes in detail J1-Jumbo (a 178B parametric model) and J1-Large (a 7B parametric model) from AI21 Labs, focusing on their architecture and training, and evaluating their performance relative to GPT-3. Evaluate complexity as well as zero and few-shot learning. |
| OPT                | [paper](https://arxiv.org/pdf/2205.01068.pdf)                | The article presents the Open Pretraining Transformers (OPT), a set of pretraining transformers for decoders only, with parameters ranging from 125 to 175 million, and aims to share these transformers with interested researchers in a comprehensive and responsible way, showing that OPT-175B is comparable to the GPT-31, with one-seventh the carbon footprint of the GPT-31 required for its development. |
| BLOOM              | [paper](https://arxiv.org/pdf/2211.05100.pdf)                | Most large language models are developed by resourceful organizations and are often not available to the public. As a step towards democratizing this powerful technology, the article introduces BLOOM, an open language model with 176B parameters designed and built by hundreds of researchers working collaboratively.BLOOM achieves competitive performance in a wide range of benchmarks and is even more powerful when fine-tuned with multitasking hints. |
| GPT 3.0            | [paper](https://arxiv.org/pdf/2005.14165.pdf)                | The article points out the great strides made in task-specific fine-tuning, but the need for task-specific fine-tuning datasets of thousands or tens of thousands of instances remains. So the article shows that extending the language model can dramatically improve performance on task-independent, few instances, sometimes rivaling even previous state-of-the-art fine-tuning methods. |
| LaMDA              | [paper](https://arxiv.org/pdf/2201.08239.pdf)                | The article demonstrates that fine-tuning using annotated data and enabling models to refer to external sources of knowledge can significantly improve the two key challenges of safety and factual basis. Safety involves ensuring that the model's responses are consistent with a range of human values, such as preventing harmful suggestions and unfair bias. Factual basis includes enabling models to consult external knowledge sources such as information retrieval systems, language translators, and calculators. |
| GLM                | [paper](https://arxiv.org/pdf/2210.02414.pdf)                | The article mainly introduces the training process of GLM-130B, including its design choices, training strategies for efficiency and stability, and engineering work. The article attempts to open source a 100B-scale model that is at least as good as GPT-3, and reveals how to successfully pre-train a model of this size. |
| LLaMA              | [paper](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/) | Proprietary datasets are difficult to access, and the paper presents LLaMA, a collection of underlying language models with parameters ranging from 7B to 65B. The researchers train LLaMA's models on trillions of tokens and show that it is possible to train state-of-the-art models exclusively using publicly available datasets without resorting to proprietary and inaccessible datasets. |
| GPT-NeoX           | [paper](https://arxiv.org/pdf/2204.06745.pdf)                | The main contribution of this article is to open source the weight file. |
| UL2                | [paper](https://arxiv.org/pdf/2205.05131v1.pdf)              | The paper proposes a unified pre-trained model framework that is generally effective across datasets and settings. The researchers first separate the architectural prototype from the pre-training target‚Äîtwo concepts that are often confused. Next, we propose a general and unified view of self-supervision in NLP and show how converting different pre-trained objectives to each other and interpolating between different objectives can be effective. We then propose hybrid denoisers (MoD), a pre-training objective that combines different pre-training paradigms. Furthermore, the concept of mode switching is introduced, where downstream fine-tuning is associated with a specific pre-training scheme. |
| ÈπèÁ®ã.ÁõòÂè§Œ±         | [paper](https://arxiv.org/pdf/2104.12369.pdf)                | The paper demonstrates the practice of training a large-scale autoregressive language model called PanGu-Œ± with up to 200 billion parameters. The researchers empirically tested the generation capabilities of PanGu-Œ± in various scenarios such as text summarization, question answering, and dialogue generation. Additionally, the effect of model size on few-shot performance in various Chinese NLP tasks is investigated. |
| T5                 | [paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf)  | In this paper, researchers explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. |
| CPM-Bee            | [paper](https://arxiv.org/pdf/2012.00413.pdf)                | Applying GPT-3 to solve Chinese NLP tasks is still challenging because the training corpus of GPT-3 is mainly English and the parameters are not public. In this technical report, the researchers released a Chinese pre-trained language model (CPM) for generative pre-training on large-scale Chinese training data. With 2.6 billion parameters and 100GB of Chinese training data, CPM is the largest Chinese pre-trained language model that can facilitate multiple downstream Chinese NLP tasks such as dialogue, paper generation, proximity testing, and language understanding. Extensive experiments show that CPM achieves excellent performance on many NLP tasks in the setting of few-shot (or even zero-shot) learning. |
| rwkv-4             | [paper](https://arxiv.org/pdf/2305.13048.pdf)                | Compared to Transformers, Recurrent Neural Networks (RNNs) exhibit linear scaling in terms of memory and computational requirements, but due to parallelization and scalability limitations, it is difficult to achieve the same performance as Transformers. The article proposes a novel model architecture, Reception Weighted Key-Value (RWKV), which combines efficient parallel training of Transformers with efficient inference of RNNs. Experiments show that RWKV performs comparable to a Transformer of similar size, suggesting that future work can leverage this architecture to create more efficient models. |

| Model             | Link                                                         | Insight                                                      |
| ----------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Flan-PaLM/Flan-T5 | [paper](https://arxiv.org/pdf/2210.11416.pdf)                | The paper explore instruction fine-tuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. Instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation, RealToxicityPrompts). |
| BLOOMZ            | [paper](https://arxiv.org/pdf/2211.01786.pdf)                | The paper find fine-tuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. |
| InstructGPT       | [paper](https://arxiv.org/pdf/2203.02155.pdf)                | Large language models can generate output that is unrealistic, toxic, or unhelpful to users. The paper demonstrate a path to align language models with users' intent on a variety of tasks by fine-tuning them based on human feedback. |
| Galactica         | [paper](https://arxiv.org/pdf/2211.09085.pdf)                | The paper introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. reserchers train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. |
| Gopher            | [paper](https://storage.googleapis.com/deepmind-media/research/language-research/Training%20Gopher.pdf) | The article analyzes the performance of Transformer-based language models at various model scales‚Äîfrom models with tens of millions of parameters to a 280 billion-parameter model called Gopher. The models were evaluated on 152 different tasks, achieving state-of-the-art performance on most of them. |
| Chinchilla        | [paper](https://arxiv.org/pdf/2203.15556.pdf)                | The paper finds that current large-scale language model training is significantly undertrained, which is the result of a recent focus on scaling up language models while keeping the amount of training data constant. For computationally optimal training, the model size and the number of training tokens should scale equally: for every doubling of the model size the number of training tokens should also double. |
| T0                | [paper](https://arxiv.org/pdf/2110.08207.pdf)                | Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks. To test the question that zero-shot can generalization instead be directly induced by explicit multitask learning, the paper develop a system for easily mapping any natural language tasks into a human-readable prompted form. |
| Orca              | [paper](https://arxiv.org/pdf/2306.02707.pdf)                | Recent research has focused on enhancing the capability of smaller models through imitation learning, drawing on the outputs generated by large foundation models (LFMs). A number of issues impact the quality of these models, ranging from limited imitation signals from shallow LFM outputs; small scale homogeneous training data; and most notably a lack of rigorous evaluation resulting in overestimating the small model‚Äôs capability as they tend to learn to imitate the style, but not the reasoning process of LFMs. Orca addresses these challenges by learning to imitate the reasoning process of LFMs. |
| UltraLM           | [paper](about:blank)                                         | This paper aims to improve the upper bound of open-source models further. They first provide a systematically designed, diverse, informative, large-scale dataset of instructional conversations, UltraChat, which does not involve human queries. Their objective is to capture the breadth of interactions that a human might have with an AI assistant and employs a comprehensive framework to generate multi-turn conversation iteratively. |
| Guanaco           | [paper](https://arxiv.org/pdf/2305.14314.pdf)                | The paper present QLORA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. |
| Baize             | [paper](https://arxiv.org/pdf/2304.01196.pdf)                | The paper propose a pipeline that can automatically generate a high-quality multi-turn chat corpus by leveraging ChatGPT to engage in a conversation with itself. Subsequently, they employ parameter-efficient tuning to enhance LLaMA, an open-source large language model. The resulting model, named Baize, demonstrates good performance in multi-turn dialogues with guardrails that minimize potential risks. |



## LLM Extensions

To make the large language model more interesting, we looked for some LLM-based extensions that are interesting and worth checking out.

* [MindsDB](https://github.com/mindsdb/mindsdb): A project that combines a large language model with the creation and manipulation of databases
  * MindsDB is a Server for Artificial Intelligence Logic, enabling developers to ship AI powered systems from prototyping & experimentation to production in a fast & scalable way.
* [ai](https://github.com/vercel-labs/ai): LLM Combined with code generation, this project builds React, Svelte, Vue, and Solid apps with AI
  * The Vercel AI SDK is a library for building AI-powered streaming text and chat UIs.
* [poerful-llms](https://github.com/howl-anderson/unlocking-the-power-of-llms): Use prompts and chains to make chatgpt a fantastic productivity tool
  * Use ChatGPT (which will also include Google Bard) for a variety of NLP and non-NLP tasks, including text generation, colorization, and more!
* [engshell](https://github.com/emcf/engshell):  A shell command that plays with various operating systems with the help of LLM
  * An English-language shell for any OS, powered by LLMs
* [Superagent](https://github.com/homanp/superagent): Building, Deploying and Managing LLM Supported Agents
  * Superagent is a powerful tool that simplifies the configuration and deployment of LLM (Large Language Model) Agents to production. It provides a range of features and functionalities to make it easier for developers to build, manage and deploy AI agents to production including features such as built in memory and document retrieval via vector dbs, powerful tools, webhooks, cron jobs etc.
* [SillyTavern](https://github.com/SillyTavern/SillyTavern): LLM front-end for advanced users
  * Mobile-friendly, Multi-API (KoboldAI/CPP, Horde, NovelAI, Ooba, OpenAI+proxies, WindowAI(Claude!)), VN-like Waifu Mode, Horde SD, System TTS, WorldInfo (lorebooks), customizable UI, auto-translate, and more prompt options than you'd ever want or need. Optional Extras server for more SD/TTS options + ChromaDB/Summarize.
* [flux](https://github.com/paradigmxyz/flux): Graph-based LLM dynamics tool for parallel exploration of many completed projects
  * Flux is a power tool for interacting with large language models (LLMs) that generates multiple completions per prompt in a tree structure and lets you explore the best ones in parallel.
* [code-review-gpt](https://github.com/mattzcarey/code-review-gpt): Code Reviewer with LLM Modeling
  * Code Review GPT uses Large Language Models to review code in your CI/CD pipeline. It helps streamline the code review process by providing feedback on code that may have issues or areas for improvement.
* [vim-ai](https://github.com/madox2/vim-ai): Artificial intelligence code assistant for Vim. OpenAI and ChatGPT plugins for Vim and Neovim
  * This plugin adds Artificial Intelligence (AI) capabilities to your Vim and Neovim. You can generate code, edit text, or have an interactive conversation with GPT models, all powered by OpenAI's API.
* [readme-ai](https://github.com/eli64s/readme-ai): Generate nice README.md files from the terminal. Supported by OpenAI's GPT LLM
  * README-AI is a powerful, user-friendly command-line tool that generates extensive README markdown documents for your software and data projects. By providing a remote repository URL or directory path to your codebase, this tool will document your entire project, leveraging the capabilities of large language models and OpenAI's GPT APIs.
* [mindflow](https://github.com/mindflowai/mindflow): AI-enabled CLI git wrapper, sample code generator, chat history manager, and code search engine to streamline your development workflow
  * The ChatGPT-powered swiss army knife for the modern developer! We provide an AI-powered CLI git wrapper, boilerplate code generator, code search engine, a conversation history manager, and much more!
* [LangChain](https://github.com/langchain-ai/langchain): Building Applications with LLM through Composability
  * Large language models (LLMs) are emerging as a transformative technology, enabling developers to build applications that they previously could not. However, using these LLMs in isolation is often insufficient for creating a truly powerful app - the real power comes when you can combine them with other sources of computation or knowledge.
* [haystack](https://github.com/deepset-ai/haystack): Haystack is an open source NLP framework for interacting with data using Transformer models and LLMs (GPT-4, Falcon, etc.)
  * Haystack is an end-to-end NLP framework that enables you to build NLP applications powered by LLMs, Transformer models, vector search and more. Whether you want to perform question answering, answer generation, semantic document search, or build tools that are capable of complex decision making and query resolution, you can use the state-of-the-art NLP models with Haystack to build end-to-end NLP applications solving your use case.
* [PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP): An easy-to-use and powerful natural language processing development library.
* [postgresml](https://github.com/postgresml/postgresml): A database of artificial intelligence applications
  * PostgresML is a machine learning extension to PostgreSQL that enables you to perform training and inference on text and tabular data using SQL queries. With PostgresML, you can seamlessly integrate machine learning models into your PostgreSQL database and harness the power of cutting-edge algorithms to process data efficiently.
* [anything-llm](https://github.com/Mintplex-Labs/anything-llm): A full-stack app that transforms any document into a smart chatbot with a sleek UI and an easier way to manage your workspace
  * A full-stack application and tool suite that enables you to turn any document, resource, or piece of content into a piece of data that any LLM can use as reference during chatting. This application runs with very minimal overhead as by default the LLM and vectorDB are hosted remotely, but can be swapped for local instances. Currently this project supports Pinecone, ChromaDB & more for vector storage and OpenAI for LLM/chatting.
* [500+ best AI tools](https://vaulted-polonium-23c.notion.site/500-Best-AI-Tools-e954b36bf688404ababf74a13f98d126): Collection of numerous AI productivity tools not limited to LLM models